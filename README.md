# K-Nearest-Neighbor
## Welcome to the KNN Project! This project closely follows the structure of the lecture but with a different dataset. The goal is to implement the K Nearest Neighbors algorithm and evaluate its performance.
## In this project, you will apply the K Nearest Neighbors (KNN) algorithm to classify data. The dataset is artificial, and you will perform exploratory data analysis (EDA), data standardization, train-test splitting, and model evaluation.

# Libraries and Data
### pandas,  numpy, matplotlib.pyplot, seaborn (%matplotlib inline)
### Artificial dataset 

# EDA (Exploratory Data Analysis)
### Use a a large pairplot will to explore the relationships between the features.

# Standardization
### Standardize the features to ensure that all variables contribute equally to the distance calculations - sklearn.preprocessing.

# Train-Test Split
### Split the data into training and testing sets - sk-learn.model_selection.

# Choosing a K Value
### Use the 'elbow method' to find the optimal K value by plotting the error rate against K values.

# Model Training
### Train a KNN model with n_neighbors is the optimal K value found from the previous step.

# Predictions and Evaluation
### Evaluate the model using a confusion matrix and classification report.
